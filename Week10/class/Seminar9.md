# Seminar 9: Generalization and Function Approximation

In this seminar, we will practice solving reinforcement problems by using function approximation. Specifically, we will consider two exercises:
* The Mountain Car example: linear function approximator
* The Atari 2600 games: nonlinear approximator with experience replay

Before starting looking into our exercises, we provie a summary of some basic concepts. 

## A quick summary of some basic concepts

### On-policy vs off-policy methods

On-policy methods aim to estimate the value of the policy that is used for control. Examples of on-policy methods are:  
- On-policy MC control 
- On-policy TD control: SARSA

The updates of action estimates according to SARSA are of the following form:

<img src="https://latex.codecogs.com/svg.latex?\Large&space;Q(s_t,a_t){\leftarrow}Q(s_t,a_t)+\alpha[r_{t+1}+{\gamma}Q(s_{t+1},a_{t+1})-Q(s_t,a_t)]" title="x" />

An on-policy method updates action value estimates using the values of next state-action pairs where **the next actions are generated by the current policy**. 

In off-policy methods, the behaviour policy that generates actions may be unrelated to the target policy that's estimated and improved. Examples of off-policy methods are: 
- Off-policy MC control
- Off-policy TD control: Q-learning

The updates of action estimates according to Q-learning are of the following form:

 <img src="https://latex.codecogs.com/svg.latex?\Large&space;Q(s_t,a_t){\leftarrow}Q(s_t,a_t)+\alpha[r_{t+1}+{\gamma}\max_{a}Q(s_{t+1},a})-Q(s_t,a_t)]" title="x" />

Q-learning updates action value estimates using the values of next state-action pairs where **the next actions are generated by a greedy policy**.   That is to say,  the action-state values are estimated under the assumption of a greedy policy despite the fact that it is not following a greedy policy. 

The advantage of off-policy methods is that the estimation policy may be greedy/deterministic while the behaviour policy is stochastic such that the optimal policy is found during exploration.  In comparison, on-policy methods maintain exploration of non-greedy by having soft policies, i.e., <img src="https://latex.codecogs.com/svg.latex?\Large&space;\pi(s,a)>0" title="x" /> for all <img src="https://latex.codecogs.com/svg.latex?\Large&space;s\in{\mathcal{S}}" title="x" /> and all <img src="https://latex.codecogs.com/svg.latex?\Large&space;a\in{\mathcal{A}(s)}" title="x" />. 

### Online vs offline methods

In general, online learning is learning as new data becomes available. 
An update of state/state-action value is made after each action is taken and a new observation is obtained. 
Many online learning methods can also be made offline. 
The following classifications are based on the version of algorithms taught in this course: 
- TD(0)
- Sarsa
- Q-learning
- Eligibility traces / the backward view of TD(<img src="https://latex.codecogs.com/svg.latex?\Large&space;\lambda" title="x" />)  

Off-line learning means learning from a static dataset. 
In our case, learning is done by the end of an entire episode using data collected during the episode. Examples are:
- On-policy and off-policy MC methods
- n-Step TD
- Offline TD(<img src="https://latex.codecogs.com/svg.latex?\Large&space;\lambda" title="x" />) / the forward view of TD(<img src="https://latex.codecogs.com/svg.latex?\Large&space;\lambda" title="x" />) 


### Recap on value function approximation 

What you have learned in the lecture: 

<img  src="https://github.com/lse-st449/lectures/raw/master/Week10/class/graphs/linear_feature.png"  width="550"/>

### Control with function approximation

<img  src="https://github.com/lse-st449/lectures/raw/master/Week10/class/graphs/control_approx.png"  width="650"/>

## The Mountain Car Problem 
The Mountain Car problem is described in **Example 10.1** (2nd Edition) in Sutton & Barto. 
The basic idea is driving the car to reach the goal position by building up momentum. 
We will use the existing environment in OpenAI Gym (ID: MountainCar-v0).

<img  src="https://github.com/lse-st449/lectures/raw/master/Week10/class/graphs/Screenshot_mcar.png"  width="500"/>

- The goal is to reach the flag position
- Actions = [push left (0), no push (1), push right (2)]
- Reward is -1 on all time steps until the car reaches its goal
- The states have two dimensions: state = [position, velocity]
- The state space is continuous


### Feature engineering 
There are many ways to create the feature vector of a state. We can either use handcrafted or learned features. For instance, AlphaGo uses handcrafted features:

<img  src="https://github.com/lse-st449/lectures/raw/master/Week10/class/graphs/alpha_go_features.png"  width="650"/>

In reinforcement learning, the commonly used methods to extract features include tile-coding, kernel functions, or neural networks. 
In this exercise, we will create the feature vectors using kernel methods. 

The RBF kernel is defined as
<img src="https://latex.codecogs.com/svg.latex?\Large&space;K(x,x')=\exp(-\gamma||x-x'||^2)" title="x"/>

For any positive definite kernel <img src="https://latex.codecogs.com/svg.latex?\Large&space;K(x,x')" title="x"/> in recursive Kernel Hilbert space (RKHS), we have
<img src="https://latex.codecogs.com/svg.latex?\Large&space;K(x,x')=\phi(x)^{\top}{\phi(x')}" title="x"/>, where <img src="https://latex.codecogs.com/svg.latex?\Large&space;\phi" title="x"/> 
is a feature map.

#### Normalize data
As the RBF kernel assumes all features are centred around 0 and have variance in the same order, we will normalize 
the data as the first step. For the task of normalization and feature engineering, we will use the **scikit-learn** library.

Assumed that you have already installed Numpy and Scipy, install sklearn using pip
```
pip install -U scikit-learn
```

As this method falls into the category of online learning, we will define a normalizer and feature extractor using 
some pre-training data as follows:

We randomly sample 10000 observations as pre-training data for normalization.
```
sample_size = 10000

obs_samples = np.array([env.observation_space.sample() for i in range(sample_size)])
```

The following code creates an instance of the sklearn StandardScaler class which has a method `.fit()`that computes the mean and standard deviation to be used for later scaling.

```
scaler = sklearn.preprocessing.StandardScaler().fit(obs_samples)
```
When a new observation comes in, we can simply call 
```
scaled_state = scaler.transform([state])
```
to normalize the latest observed state.  

#### Create features 
The `sklearn.pipeline.FeatureUnion` class concatenates results of multiple transformer objects. This class allows you to combine 
multiple feature extraction mechanisms into a single transformer. We will choose 4 RBF kernels with different gamma values and an 
output dimensionality of 100.

```
feature_dim = 100

featuriser = sklearn.pipeline.FeatureUnion(
[("rbf1", RBFSampler(gamma=5.0, n_components=feature_dim)),
("rbf2", RBFSampler(gamma=2.0, n_components=feature_dim)),
("rbf3", RBFSampler(gamma=1.0, n_components=feature_dim)),
("rbf4", RBFSampler(gamma=0.5, n_components=feature_dim))])
```

We now fit all the transformers using the scaled samples by the method `.fit()` under this class,

```
featuriser.fit(scaler.transform(obs_samples))
```

The `.transform()` method of the `scaler` returns the centred and scaled samples.
```
features = featuriser.transform(scaled)
```

Now the `scaler` and `featuriser` are ready for later use.  When we obtain a new state, we can transform it to feature vectors easily.


### Control with function approximation

We approximate the action-value function by a linear combination of features, i.e.,

<img src="https://latex.codecogs.com/svg.latex?\Large&space;\hat{Q}(s,a,\mathbf{w})=\phi(s,a)^{\top}\mathbf{w}" title="x"/>


The objective function is the MSE between the true action value <img src="https://latex.codecogs.com/svg.latex?\Large&space;Q(s,a)" title="x"/> and 
<img src="https://latex.codecogs.com/svg.latex?\Large&space;\hat{Q}(s,a,\mathbf{w})" title="x"/>:

<img src="https://latex.codecogs.com/svg.latex?\Large&space;\mathcal{L(\mathbf{w})}=\frac{1}{2}\mathbb{E}_{s,a,r,s'}[(Q(s,a)-\hat{Q}(s,a,\mathbf{w}))^2]" title="x"/>

  
Differentiating the objective w.r.t. the parameter <img src="https://latex.codecogs.com/svg.latex?\Large&space;\mathbf{w}" title="x"/>, we obtain the parameter update

<img src="https://latex.codecogs.com/svg.latex?\Large&space;\mathbf{w}_{t+1}=\mathbf{w}_t+{\eta}(Q(s_t,a_t)-\hat{Q}(s_t,a_t,\mathbf{w}_t))\phi(s_t,a_t)" title="x"/>

Unfortunately, we don't have access to the true <img src="https://latex.codecogs.com/svg.latex?\Large&space;Q(s,a)" title="x"/> in practice. We therefore substitute a target for it.

We can substituting with a SARSA target

<img src="https://latex.codecogs.com/svg.latex?\Large&space;Q(s_t,a_t)=r_{t+1}+\gamma\hat{Q}(s_{t+1},a_{t+1},\mathbf{w}_t)" title="x"/>


or a Q-learning target,

<img src="https://latex.codecogs.com/svg.latex?\Large&space;Q(s_t,a_t)=r_{t+1}+\gamma\max_{a'}\hat{Q}(s_{t+1},a',\mathbf{w}_t)" title="x"/>

In the following implementation, we choose the SARSA target as a substitution to the true state-action values. The update for parameter  <img src="https://latex.codecogs.com/svg.latex?\Large&space;\mathbf{w}" title="x"/> is therefore 

<img src="https://latex.codecogs.com/svg.latex?\Large&space;\mathbf{w}_{t+1}=\mathbf{w}_t+{\eta}(r_{t+1}+\gamma\hat{Q}(s_{t+1},a_{t+1},\mathbf{w}_t)-\hat{Q}(s_t,a_t,\mathbf{w}_t))\phi(s_t,a_t)" title="x"/>

#### Implementation of the approximation function
```
class Approximator:  
    def __init__(self, obs, dim, n_a, alpha, discount):  
        self.discount = discount  
        self.alpha = alpha  
        self.n_actions = n_a  
        self.obs_samples = obs  
        self.feature_dim = dim  
        self.w_size = 4 * self.feature_dim  
        self.w = np.zeros((self.n_actions, self.w_size))  
        self.scaler = None  
        self.featuriser = None  
        self.initialised = False  
  
    def initialise_scaler_featuriser(self):  
        self.scaler = sklearn.preprocessing.StandardScaler().fit(self.obs_samples)  
        self.featuriser = sklearn.pipeline.FeatureUnion(
                          [("rbf1", RBFSampler(gamma=5.0, n_components=self.feature_dim)),
                          ("rbf2", RBFSampler(gamma=2.0, n_components=self.feature_dim)),  
                          ("rbf3", RBFSampler(gamma=1.0, n_components=self.feature_dim)),  
                          ("rbf4", RBFSampler(gamma=0.5, n_components=self.feature_dim))])  
        self.featuriser.fit(self.scaler.transform(self.obs_samples))  
        self.initialised = True  
  
    @property  
    def get_w(self):  
        return self.w  
  
    def feature_transformation(self, state):  
        if not self.initialised:  
            self.initialise_scaler_featuriser()  
  
        scaled = self.scaler.transform([state])  
        features = self.featuriser.transform(scaled)  
        return features  
  
     # linear features method 
     def action_value_estimator(self, features, a):  
         return np.inner(features, self.w[a])  
  
    # minimising MSE between q(replaced by sarsa target) and q_hat  
     def update_w(self, r, q, next_q, features, a):  
         target = r + self.discount * next_q  
         sarsa_error = target - q  
         w_gradient = self.alpha * sarsa_error * features  
         self.w[a] = self.w[a] + w_gradient  
  
     def cost_to_go(self, state):  
         features = self.feature_transformation(state)  
         v_s = []  
         for i in range(self.n_actions):  
             v_s.append(self.action_value_estimator(features, i))  
         return - np.max(v_s)
```
The cost to go function is defined as 
<img src="https://latex.codecogs.com/svg.latex?\Large&space;f=-\max_{a}Q_t(s,a)" title="x"/>

#### Solve the problem
Randomly sample 10000 observations, create an instance of the state-action function approximator and initialize the normaliser and feature extractor: 
```
sample_size = 10000  
obs_samples = np.array([env.observation_space.sample() for i in range(sample_size)])  

# create an instance of approximator and initialize the normalizer and feature extractor 
estimator = Approximator(obs_samples, dim, n_actions, alpha, gamma)  
estimator.initialise_scaler_featuriser()  
```

Play the game and do online updates for parameter <img src="https://latex.codecogs.com/svg.latex?\Large&space;\mathbf{w}" title="x"/>, 
```
for i in range(1, episodes + 1):  
    state = env.reset()  
    done = False  
    a = env.action_space.sample()  
  
    while not done:  
        next_state, r, done, _ = env.step(a)  
  
        # compute q_sa for the current state-action pair
        features = estimator.feature_transformation(state)  
        q_sa = estimator.action_value_estimator(features, a)  
  
        # Turn the next state into features
        next_features = estimator.feature_transformation(next_state)  
        
        # compute all actions in the next state for optimal policy
        q_values = []  
        for j in actions:  
            q_values.append(estimator.action_value_estimator(next_features, j))  
  
        next_a = epsilon_greedy_policy(epsilon, actions, q_values)  
        next_q_sa = estimator.action_value_estimator(next_features, next_a)  
  
        # update weights for current (state, action)  
        estimator.update_w(r, q_sa, next_q_sa, features, a)  
  
        a = next_a  
        state = next_state
```

### Results

The first plot is after 4 episodes, whilst the second one is after 100 episodes

<img  src="https://github.com/lse-st449/lectures/raw/master/Week10/class/graphs/mountain_car4epi.png"  width="550"/>  <img  src="https://github.com/lse-st449/lectures/raw/master/Week10/class/graphs/mountain_car.png"  width="550"/>

  

As we can see, the optimal policy after 4 episodes of training is not good enough to solve the task.


<a href="https://github.com/lse-st449/lectures/raw/master/Week10/class/graphs/4episodes.mp4
" target="_blank"><img src="https://github.com/lse-st449/lectures/raw/master/Week10/class/graphs/Screenshot_mcar.png" 
alt="MountainCar100episodes" width="500" height="400" border="10" /></a>

  
Policy after 100 episodes,

<a href="https://github.com/lse-st449/lectures/raw/master/Week10/class/graphs/100episodes.mp4
" target="_blank"><img src="https://github.com/lse-st449/lectures/raw/master/Week10/class/graphs/Screenshot_mcar.png" 
alt="MountainCar100episodes" width="500" height="400" border="10" /></a>


To replicate the results in the Sutton & Barto book, we observe that the action-values indeed display a spiral shape.

The left plot is after 1000 episodes, whilst the right one is after 9000 episodes,

<img  src="https://github.com/lse-st449/lectures/raw/master/Week10/class/graphs/mountain_car1000epi.png"  width="550"/>  <img  src="https://github.com/lse-st449/lectures/raw/master/Week10/class/graphs/mountain_car9000epi.png"  width="550"/>


## Deep Q-Networks (DQN) with experience replay

This exercise aims to replicate the results from the paper [ Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236) published in Nature. This paper develops a DQN that can learn successful policy directly from high-dimensional inputs using end-to-end reinforcement learning. We will implement the algorithm and test it on the Atari game Breakout.

![](atari_breakout.gif)

### Frame-skipping

![](https://github.com/lse-st449/lectures/raw/master/Week10/class/graphs/breakout_screenshots.png)


These are 30 consecutive frames extracted from the game. If you execute an action in each frame, then there's **no** frame skipping.

  

In the **Methods** section of the paper, it is said:

>… the agent sees and selects actions on every kth frame instead of every frame, and its last action is repeated on skipped frames. … We use k = 4 for all games except Space Invaders where we noticed that using k = 4 makes the lasers invisible because of the period at which they blink. We used k = 3 to make the lasers visible …

When k = 4, the skipped frames are indicted with an "X" as below:

![](https://github.com/lse-st449/lectures/raw/master/Week10/class/graphs/breakout_subsampled.png)

  
Denote the frames left (the non-skipped ones) as 
<img src="https://latex.codecogs.com/svg.latex?\Large&space;\{x_1,x_2,x_3,x_4,x_5,x_6,x_7\}" title="x" />. The states, also the inputs of the Q-network, are <img src="https://latex.codecogs.com/svg.latex?\Large&space;S_1=\{x_1,x_2,x_3,x_4\}" title="x" />, <img src="https://latex.codecogs.com/svg.latex?\Large&space;S_2=\{x_2,x_3,x_4,x_5\}" title="x" /> and so on.


### Preprocessing

As stated in the paper:

>Working directly with raw Atari frames, which are 210×160 pixel images with a 128 color palette, can be computationally demanding, so we apply a basic preprocessing step […]. The raw frames are preprocessed by first converting their RGB representation to gray-scale and down-sampling it to a 110×84 image.

  

After down-sampling it to 110 x 84, the frames are then re-scaled to 84 x 84. Therefore, the inputs of the Q-network have a shape of 84 x 84 x 4.

  

Here's the code:

```
def preprocess(x, frame_height=84, frame_width=84):
    cropped_x = tf.image.crop_to_bounding_box(x, 20, 0, 180, 160) # the original 210 * 160 pixel images
    downsize_x = tf.image.resize_images(cropped_x, size=[frame_height,frame_width],method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)
    gray_x = tf.image.rgb_to_grayscale(downsize_x)
    return tf.squeeze(gray_x)
```

In the paper, they also clipped all the positive rewards at 1 and all negative rewards at -1 and left 0 rewards unchanged. This is because the scale of scores varies from game to game. As explained,

> ...Clipping the rewards in this manner limits the scale of the error derivatives and makes it easier to use the same learning rate across multiple games.

  

We write the following function to complete the job:

```
def clipping_reward(r):
    return np.sign(r)
```

`np.sign(x)` returns -1 if x < 0, 0 if x=0, and 1 if x > 0


### Experience replay

Experience replay is a method that aims to prevent the network from diverging. The paper offered us a comparison of results with experience reply and without cross multiple games.


<img  src="https://github.com/lse-st449/lectures/raw/master/Week10/class/graphs/replay_improvement.png"  width="550"/> 

  

#### Basic idea of experience replay in Atari games

The basic idea is that you choose an action, play one step in the game, and then you add all the relevant information 
(next state, action taken, reward and whether the next state is terminal) to a finite-size memory you have allocated. 
Then you update the network using random samples from the memory.

#### Add to memory

Create a class `class Memory` and initialise 4 arrays in the constructor to store observations, actions, rewards and whether the states are terminals.
```
class Memory:
    def __init__(self, size, obs_dims, batch_size, history_length):
        self.memory_size = size
        self.observation_dims = obs_dims
        self.batch_size = batch_size
        self.history_length = history_length
        self.state_shape = [self.history_length, self.observation_dims[0], self.observation_dims[1]]
        self.actions = np.empty(self.memory_size, dtype=np.uint8)
        self.rewards = np.empty(self.memory_size, dtype=np.int8)
        self.observations = np.empty([self.memory_size] + self.observation_dims, dtype=np.uint8)
        self.terminals = np.empty(self.memory_size, dtype=np.bool)
        self.__count = 0
        self.__current = 0

        self.prestates = np.empty([self.batch_size] + self.state_shape, dtype=np.uint8)
        self.nextstates = np.empty([self.batch_size] + self.state_shape, dtype=np.uint8)

```
A method in the class to store observations:
```
def add(self, next_state, action, r, terminal):
        processed_state = preprocess(next_state)
        clipped_r = clipping_reward(r)

        self.actions[self.__current] = action
        self.rewards[self.__current] = clipped_r
        self.observations[self.__current, ...] = processed_state
        self.terminals[self.__current] = terminal
        # maximum is memory size
        self.__count = max(self.__count, self.__current + 1)
        # a finite memory size
        self.__current = (self.__current + 1) % self.memory_size
```

#### Initialization

Before training the network, we store some training data in memory to start with. 
This is done by choosing a random action at each step, executing the action in the step function, and then adding all the relevant 
information to memory. Repeat this process until the memory is prefilled with a certain size.


**Note** that experience replay takes a lot of memory. A single preprocessed frame requires a minimum of around 8.5KB to be stored in RAM. 
In the paper, the size of memory is 1 million, which means 9GB memory required just for storing the frames. To mitigate the memory demand, we should

- store individual frames rather than states to avoid storing overlapped frames as a state is 4 frames stacked together which requires 4 times more memory

- store frames using the `.unit8` data type as `.float32` type uses 4 times as much memory and `.float64` uses 8 times as much

- never copy the frames

- if Python still gives you a `MemoryError`, you will have to reduce the size of memory

Another hacky aspect of the paper is to treat life losing in the initialisation as terminal. 
Because there is no penalty for losing a life given by the environment, the agent doesn't know that it is bad to lose a life. 
Therefore, we terminates the episode manually to help the agent to learn faster.

```
def initialise(self, size, env):
    while True:
        env.reset()
        is_done = False
        initial_lives = 5
        while True:
            if is_done or self.__count == size:
                break
            action = env.action_space.sample()
            next_state, r, is_done, info = env.step(action)
            current_lives = info['ale.lives']
            if current_lives < initial_lives:
                is_done = True
                r = 0.0
            self.add(next_state, action, r, is_done)
         if self.__count == size:
             break
```

#### Training process

We will randomly sample a minibatch of states, actions, rewards and terminals from the experience for each update.

Firstly, in the following helper function, we stack 4 frames stored in the observation array to form one state:

```
def get_states_in_training(self, idx, frames):
        next_states = frames[idx - self.history_length:idx]
        return tf.stack(next_states)
```

Where the variable `self.history_length` is the number of most recent frames used as an input.

Finally, we can sample from memory

```
def sample(self):
    indexes = []
    while len(indexes) < self.batch_size:
        while True:
            index = np.random.randint(self.history_length, self.__count - 1)
            if index < self.history_length:
                continue
            if self.__current <= index <= self.__current + self.history_length:
                continue
            if self.terminals[index - self.history_length:index].any():
                continue
            break

        self.prestates[len(indexes), ...] = self.get_state(index - 1)
        self.nextstates[len(indexes), ...] = self.get_state(index)
        indexes.append(index)

    action = self.actions[indexes]
    rewards = self.rewards[indexes]
    terminals = self.terminals[indexes]
    return self.prestates, action, rewards, terminals, self.nextstates
```

## Homework: Q-network, Annealing-epsilon-greedy policy and Huber loss in TensorFlow

  
### Q-network

Based on the description of the network in the paper, construct a Q-network:

> The input to the neural network consists of an 84 x 84 x 4 image produced by the preprocessing map phi. 
The first hidden layer convolves 32 filters of 8 x 8 with stride 4 with the input image and applies a rectifier nonlinearity. 
The second hidden layer convolves 64 filters of 4 x 4 with stride 2, again followed by a rectifier nonlinearity. 
This is followed by a third convolutional layer that convolves 64 filters of 3 x 3 with stride 1 followed by a rectifier. 
The final hidden layer is fully-connected and consists of 512 rectifier units. 
The output layer is a fully-connected linear layer with a single output for each valid action.

**Hints**:

- Write a Python class for the network

- Initialize the layers with `kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2)`

### Annealing-epsilon-greedy policy

Write an epsilon-greedy policy function as described in the paper:

> The behavior policy during training was e-greedy with e annealed linearly from 1.0 to 0.1 over the first million frames, and fixed at 0.1 thereafter.

The epsilon schedule looks like this:

<img  src="https://github.com/lse-st449/lectures/raw/master/Week10/class/graphs/eps_anealing.png"  width="550"/> 

### Huber loss function

Implement Huber loss function, defined as

<img  src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e384efc4ae2632cb0bd714462b7c38c272098cf5" class="mwe-math-fallback-image-inline"  aria-hidden="true" style="vertical-align: -3.171ex; width:35.901ex; height:7.509ex;" alt="L_{\delta}(a)={\begin{cases}{\frac{1}{2}}{a^{2}}&amp;{\text{for}}|a|\leq\delta,\\\delta(|a|-{\frac{1}{2}}\delta),&amp;{\text{otherwise.}}\end{cases}}">
  
The blue line indicates quadratic loss and the green line shows Huber loss when <img src="https://latex.codecogs.com/svg.latex?\Large&space;\delta=1" title="x"/>:


<img  src="https://github.com/lse-st449/lectures/raw/master/Week10/class/graphs/huber_loss.png"  width="550"/> 


**Hints**:
- a = the difference between true values and predictions
- The function takes 3 arguments: true values, predictions, and delta


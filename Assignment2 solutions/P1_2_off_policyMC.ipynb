{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2_P1_2_off_policyMC.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGlsL0-vrFH0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define global variables for the problem\n",
        "\n",
        "p_h = 0.55\n",
        "p_l = 0.45\n",
        "\n",
        "prize = 1000\n",
        "c_h = 50\n",
        "c_l = 10\n",
        "\n",
        "high = 0\n",
        "low = 1\n",
        "\n",
        "# number of actions\n",
        "n_actions = 2\n",
        "actions = [high, low]\n",
        "\n",
        "# two outcomes\n",
        "win = 0\n",
        "lose = 1\n",
        "\n",
        "n_outcomes = 2\n",
        "\n",
        "# number of rounds\n",
        "d = 3\n",
        "\n",
        "# threshold\n",
        "thresh = 4\n",
        "\n",
        "# states encoded as 0, 1, ..., 2d\n",
        "states = np.arange(0,2*d+1) \n",
        "n_states = len(states)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoyZ8RIZq5GX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Game:\n",
        "    def __init__(self, init_state):\n",
        "        self.initial_state = init_state\n",
        "        self.state = self.initial_state\n",
        "        self.reward = 0.0\n",
        "        self.is_terminal = False\n",
        "\n",
        "    def outcome(self, action):\n",
        "        if action == high:\n",
        "            win = np.random.binomial(1, p_h)\n",
        "        else:\n",
        "            win = np.random.binomial(1, p_l)\n",
        "        return win\n",
        "\n",
        "    def step(self, action):\n",
        "        if action == high:\n",
        "            self.reward = -c_h\n",
        "        else:\n",
        "            self.reward = -c_l    \n",
        "      \n",
        "        is_won = self.outcome(action)\n",
        "        \n",
        "        if self.state == 2*d-1 and is_won:\n",
        "            self.state += 1\n",
        "            self.reward += prize\n",
        "            self.is_terminal = True\n",
        "        elif self.state == 1 and not is_won:\n",
        "            self.state -= 1\n",
        "            self.is_terminal = True\n",
        "        else:\n",
        "            if is_won:\n",
        "                self.state += 1\n",
        "            else:\n",
        "                self.state -= 1\n",
        "            self.is_terminal = False\n",
        "        return self.state, self.reward, self.is_terminal\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = self.initial_state\n",
        "        self.reward = 0.0\n",
        "        self.is_terminal = False\n",
        "        return self.state\n",
        "\n",
        "\n",
        "def behavior_policy():\n",
        "    return np.random.binomial(1, 0.5)\n",
        "\n",
        "\n",
        "# Target policy is the threshold policy\n",
        "def thresh_policy(state,action):\n",
        "    if state < thresh:\n",
        "      if action == high:\n",
        "        return 0.8\n",
        "      else:\n",
        "        return 0.2\n",
        "    else:\n",
        "      if action == high:\n",
        "        return 0.5\n",
        "      else:\n",
        "        return 0.5\n",
        "\n",
        "\n",
        "def policy(x):\n",
        "  pi_a = np.zeros((n_states,n_actions))\n",
        "  for i in range(len(x)):\n",
        "    pi_a[i+1, x[i]] = 1\n",
        "  return pi_a\n",
        "\n",
        "#target_policy = policy([low, high, high, low, low])\n",
        "\n",
        "target_policy = policy([low, high, high, high, high])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIhLQh9QrJE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "init_state = d\n",
        "env = Game(init_state)\n",
        "episodes = 500000\n",
        "\n",
        "q_values = np.zeros((n_states, n_actions))\n",
        "c_values = np.zeros_like(q_values)\n",
        "\n",
        "# No discount\n",
        "gamma = 1.0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICmT5NC0rYd2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(1, episodes + 1):\n",
        "  state = env.reset()\n",
        "  history = []\n",
        "  done = False\n",
        "  t = 0\n",
        "\n",
        "  while not done:\n",
        "      a = behavior_policy()\n",
        "      next_state, r, done = env.step(a)\n",
        "        \n",
        "      history.append([t, state, a, r])\n",
        "      state = next_state\n",
        "      t += 1\n",
        "\n",
        "  g = 0.0\n",
        "  w = 1.0\n",
        "\n",
        "  for t, state, action, reward in history[::-1]:\n",
        "      g = gamma * g + reward\n",
        "      c_values[state, action] += w\n",
        "      q_values[state, action] += w* (g - q_values[state, action]) / c_values[state, action]\n",
        "      w *= target_policy[state, action] / 0.5\n",
        "      if w == 0:\n",
        "         break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bwRjArQrsEh",
        "colab_type": "code",
        "outputId": "a9a3ad36-b33e-4b72-b124-698a803cf2b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "print(q_values[1:2*d,:])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ -7.45533251  45.58610987]\n",
            " [155.37501915 108.87626375]\n",
            " [284.965277   295.2753474 ]\n",
            " [460.21471044 501.45294294]\n",
            " [706.16996782 784.92463592]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DrcIOjw6Y4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9w2gf0sjfrXi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWMMHZaCgDk7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
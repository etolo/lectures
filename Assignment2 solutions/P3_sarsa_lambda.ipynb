{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2_P3_sarsa-lambda.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdCheWfTvJHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define global variables for the problem\n",
        "p_h = 0.55\n",
        "p_l = 0.45\n",
        "prize = 1000\n",
        "\n",
        "c_h = 1\n",
        "c_l = 0\n",
        "energy_full = 10\n",
        "energy_inc = 2\n",
        "energy_prob = 0.2\n",
        "\n",
        "high = 0\n",
        "low = 1\n",
        "\n",
        "# number of actions\n",
        "n_actions = 2\n",
        "actions = [high, low]\n",
        "\n",
        "# two outcomes\n",
        "win = 0\n",
        "lose = 1\n",
        "\n",
        "n_outcomes = 2\n",
        "\n",
        "# number of rounds\n",
        "d = 3\n",
        "\n",
        "\n",
        "# state consists of round win difference and energy level; we abuse this by referring to round win difference as state\n",
        "\n",
        "# states encoded as 0, 1, ..., 2d\n",
        "states = np.arange(0,2*d+1) \n",
        "n_states = len(states)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmssXYkmvTyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_outcome(action):\n",
        "    if action == high:\n",
        "        is_won = np.random.binomial(1, p_h)\n",
        "    else:\n",
        "        is_won = np.random.binomial(1, p_l)\n",
        "    return is_won\n",
        "\n",
        "\n",
        "class Game:\n",
        "    def __init__(self, init_state, full_energy, energy_prob, energy_added):\n",
        "        self.initial_state = init_state\n",
        "        self.state = self.initial_state\n",
        "        self.reward = 0.0\n",
        "        self.is_terminal = False\n",
        "        self.full_energy = energy_full\n",
        "        self.current_energy = self.full_energy\n",
        "        self.energy_prob = energy_prob\n",
        "        self.energy_added = energy_inc\n",
        "\n",
        "    def energy_recovery(self):\n",
        "        if np.random.binomial(1, self.energy_prob) == 1:\n",
        "            return min(self.full_energy, self.current_energy + self.energy_added)\n",
        "        else:\n",
        "            return self.current_energy\n",
        "\n",
        "    def step(self, action):\n",
        "        is_won = check_outcome(action)\n",
        "        if self.state == 2*d-1 and is_won:\n",
        "            self.state += 1\n",
        "            self.reward = prize\n",
        "            self.is_terminal = True\n",
        "        elif self.state == 1 and not is_won:\n",
        "            self.state -= 1\n",
        "            self.reward = 0.0\n",
        "            self.is_terminal = True\n",
        "        else:\n",
        "            if is_won and action == high:\n",
        "                self.state += 1\n",
        "                self.is_terminal = False\n",
        "            elif is_won and action == low:\n",
        "                self.state += 1\n",
        "                self.is_terminal = False\n",
        "            elif not is_won and action == high:\n",
        "                self.state -= 1\n",
        "                self.is_terminal = False\n",
        "            else:\n",
        "                self.state -= 1\n",
        "                self.is_terminal = False\n",
        "            self.reward = 0.0\n",
        "\n",
        "        if action == high:\n",
        "            cost = c_h\n",
        "        else:\n",
        "            cost = c_l\n",
        "          \n",
        "        self.current_energy = max(self.current_energy - cost, 0)\n",
        "        self.current_energy = self.energy_recovery()\n",
        "\n",
        "        return self.state, self.current_energy, self.reward, self.is_terminal\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = self.initial_state\n",
        "        self.reward = 0.0\n",
        "        self.is_terminal = False\n",
        "        self.current_energy = self.full_energy\n",
        "        return self.state, self.current_energy\n",
        "\n",
        "\n",
        "def eps_greedy_policy(qsa, energy_level, epsilon=0.1):\n",
        "    if energy_level < c_h:\n",
        "        return low\n",
        "    else:\n",
        "        if np.random.binomial(1, epsilon) == 1:\n",
        "            return np.random.choice(actions)\n",
        "        else:\n",
        "            return np.random.choice([action_ for action_, value_ in enumerate(qsa) if value_ == np.max(qsa)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aD-9lLG-vXS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "init_state = d\n",
        "episodes = 500000\n",
        "\n",
        "q_values = np.zeros((n_states, energy_full+1, n_actions))\n",
        "eligibility = np.zeros_like(q_values)\n",
        "\n",
        "#\n",
        "# Hyperparameter setting\n",
        "#\n",
        "lam = 0.9 # param for sarsa(lambda)\n",
        "gamma = 0.9 # discount factor \n",
        "alpha = 0.001 # step size\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soy02EUQvtbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = Game(init_state, energy_full, energy_prob, energy_inc)\n",
        "\n",
        "for e in range(episodes):\n",
        "    state, energy = env.reset()\n",
        "    done = False\n",
        "    action = eps_greedy_policy(q_values[state, energy, :], energy)\n",
        "        \n",
        "    while not done:\n",
        "        next_state, next_energy, r, done = env.step(action)\n",
        "        next_a = eps_greedy_policy(q_values[next_state, next_energy, :], next_energy)\n",
        "\n",
        "        delta = r + gamma * q_values[next_state, energy, next_a] - q_values[state, energy, action]\n",
        "\n",
        "        eligibility[state, energy, action] += 1\n",
        "\n",
        "        for s in range(1,2*d):\n",
        "          for en in range(energy_full+1):\n",
        "            for a in range(n_actions):\n",
        "                q_values[s, en, a] += alpha * delta * eligibility[s, en, a]\n",
        "                eligibility[s, en, a] = gamma * lam * eligibility[s, en, a]\n",
        "\n",
        "        state = next_state\n",
        "        energy = next_energy\n",
        "        action = next_a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNLWPUuRv6PI",
        "colab_type": "code",
        "outputId": "e8d7c08e-7e6f-4a96-c3e4-d6c4796ec234",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1161
        }
      },
      "source": [
        "print('Q values:\\n', q_values[1:2*d])\n",
        "\n",
        "optimal_policy = np.zeros((n_states,energy_full+1))\n",
        "\n",
        "for state in range(0,2*d+1):\n",
        "   for energy in range(0,energy_full+1):\n",
        "      optimal_policy[state,energy] = np.argmax(q_values[state,energy,:])\n",
        "\n",
        "print('Optimal policy:\\n', optimal_policy[1:2*d])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Q values:\n",
            " [[[  0.          17.02427388]\n",
            "  [ 22.43449882   1.49634712]\n",
            "  [ 29.31680089   6.54855325]\n",
            "  [ 17.54035569   5.67410212]\n",
            "  [ 28.2833465   16.21981854]\n",
            "  [ 25.60300972  49.03133304]\n",
            "  [ 64.55225125  38.30094595]\n",
            "  [ 73.27014168  53.02827195]\n",
            "  [ 99.51828826  69.79246498]\n",
            "  [ 87.79254081  60.03583213]\n",
            "  [ 82.39985448  64.8114107 ]]\n",
            "\n",
            " [[  0.          48.21269964]\n",
            "  [ 15.9118208   35.43575774]\n",
            "  [ 42.79264786  69.54584533]\n",
            "  [ 13.40610077  34.96736973]\n",
            "  [113.8895724   48.60971978]\n",
            "  [113.65028703 117.06424662]\n",
            "  [103.6602832  125.50495232]\n",
            "  [163.44958207 144.15734499]\n",
            "  [207.8795699  168.7800485 ]\n",
            "  [182.44420517 164.1674594 ]\n",
            "  [187.70972634 148.88083235]]\n",
            "\n",
            " [[  0.         107.29006038]\n",
            "  [  8.34219453  57.91490528]\n",
            "  [ 86.86110908 150.85153687]\n",
            "  [ 67.00435415  31.81690423]\n",
            "  [218.03410257 110.38556314]\n",
            "  [245.76946617 185.91898031]\n",
            "  [215.59824661 204.34024208]\n",
            "  [273.26875038 251.54451986]\n",
            "  [337.22713435 307.16759145]\n",
            "  [318.61892791 248.72948572]\n",
            "  [307.71954257 279.3807865 ]]\n",
            "\n",
            " [[  0.         206.19433151]\n",
            "  [109.8661364   25.68248678]\n",
            "  [215.45691008 289.78237862]\n",
            "  [151.15740788 108.88318784]\n",
            "  [404.79188588 217.39930942]\n",
            "  [405.34985609 354.12239466]\n",
            "  [336.94514822 380.1550156 ]\n",
            "  [472.69315627 426.34261944]\n",
            "  [516.23693869 438.25052111]\n",
            "  [487.28605028 415.0688211 ]\n",
            "  [485.83484215 434.58087517]]\n",
            "\n",
            " [[  0.         438.54502153]\n",
            "  [246.8522131    9.05912759]\n",
            "  [333.34052521 591.93012289]\n",
            "  [597.0413165  157.13118619]\n",
            "  [712.70352133 318.94456505]\n",
            "  [586.98307224 628.69418267]\n",
            "  [688.9742971  631.59626898]\n",
            "  [734.95061948 594.87420107]\n",
            "  [756.85102931 709.95174315]\n",
            "  [716.79588203 665.46583962]\n",
            "  [738.98805219 661.42751476]]]\n",
            "Optimal policy:\n",
            " [[1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}